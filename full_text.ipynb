{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "# Load and clean the DataFrame\n",
    "df = pd.read_csv(\"adh_scrape29jan25.csv\")\n",
    "df = df.drop(columns=['Unnamed: 0', 'Summary', 'PublicationDate'])\n",
    "\n",
    "# Create a new column to store the article text\n",
    "df['article_text'] = None\n",
    "\n",
    "total = len(df.index)\n",
    "successes = 0\n",
    "fails = 0\n",
    "\n",
    "def get_fulltext_link(url):\n",
    "    \"\"\"\n",
    "    Fetches the full-text article for the given URL.\n",
    "    Returns the article text if found; otherwise, returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First request: Get the page that contains the full-text link(s)\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        link_section = soup.find('div', class_='full-text-links-list')\n",
    "        \n",
    "        if link_section:\n",
    "            fulltext_link = link_section.find('a', href=True)\n",
    "            if fulltext_link:\n",
    "                # Second request: Fetch the actual article content\n",
    "                response = requests.get(fulltext_link['href'], headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                paragraphs = soup.find_all('p')\n",
    "                article_text = ' '.join([p.get_text() for p in paragraphs if p.get_text()])\n",
    "                return article_text if article_text.strip() else None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Loop over each journal (row) in the DataFrame.\n",
    "# Assumes that the URL is stored in a column named 'url'.\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['PubMedURL']  # Update the column name if necessary\n",
    "    article_text = get_fulltext_link(url)\n",
    "    \n",
    "    # Update counters based on the result.\n",
    "    if article_text:\n",
    "        successes += 1\n",
    "    else:\n",
    "        fails += 1\n",
    "    \n",
    "    # Save the result in the DataFrame (even if article_text is None)\n",
    "    df.at[idx, 'article_text'] = article_text\n",
    "\n",
    "    # Update the progress on a single line using carriage return.\n",
    "    sys.stdout.write(f\"\\rSuccess: {successes}/{total} | Fail: {fails}/{total}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Save progress every 100 rows by overwriting the same CSV file.\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        df.to_csv(\"progress.csv\", index=False)\n",
    "\n",
    "# Final save after processing all rows.\n",
    "df.to_csv(\"progress.csv\", index=False)\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# 1. Choose the model and tokenizer\n",
    "model_name = \"gpt2\"  # or choose a different model from Hugging Face\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. Prepare your corpus\n",
    "# Assuming you have a CSV file with a column 'full_text' containing your corpus\n",
    "data_file = \"bc_incidence_full_text_31jan25.csv\"\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.drop(['ExtractedStats'],axis=1).dropna().astype(str).agg(\" \".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonathan.day\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\jonathan.day\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4036b443dc794d1985525fb53c8263bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0523, 'grad_norm': 5.574575424194336, 'learning_rate': 4.912967798085292e-05, 'epoch': 0.05}\n",
      "{'loss': 2.7712, 'grad_norm': 5.774946689605713, 'learning_rate': 4.825935596170583e-05, 'epoch': 0.1}\n",
      "{'loss': 2.7284, 'grad_norm': 3.686461925506592, 'learning_rate': 4.738903394255875e-05, 'epoch': 0.16}\n",
      "{'loss': 2.6675, 'grad_norm': 3.751136302947998, 'learning_rate': 4.651871192341166e-05, 'epoch': 0.21}\n",
      "{'loss': 2.6737, 'grad_norm': 4.175460338592529, 'learning_rate': 4.564838990426458e-05, 'epoch': 0.26}\n",
      "{'loss': 2.6099, 'grad_norm': 3.9453747272491455, 'learning_rate': 4.47780678851175e-05, 'epoch': 0.31}\n",
      "{'loss': 2.5657, 'grad_norm': 3.237149715423584, 'learning_rate': 4.390774586597041e-05, 'epoch': 0.37}\n",
      "{'loss': 2.5524, 'grad_norm': 2.8351993560791016, 'learning_rate': 4.303742384682333e-05, 'epoch': 0.42}\n",
      "{'loss': 2.4988, 'grad_norm': 4.131146430969238, 'learning_rate': 4.216710182767624e-05, 'epoch': 0.47}\n",
      "{'loss': 2.4994, 'grad_norm': 2.6252501010894775, 'learning_rate': 4.129677980852916e-05, 'epoch': 0.52}\n",
      "{'loss': 2.5073, 'grad_norm': 2.743976354598999, 'learning_rate': 4.0426457789382075e-05, 'epoch': 0.57}\n",
      "{'loss': 2.4516, 'grad_norm': 2.3962035179138184, 'learning_rate': 3.9556135770234985e-05, 'epoch': 0.63}\n",
      "{'loss': 2.4407, 'grad_norm': 2.526431083679199, 'learning_rate': 3.868581375108791e-05, 'epoch': 0.68}\n",
      "{'loss': 2.4614, 'grad_norm': 3.302133083343506, 'learning_rate': 3.781549173194082e-05, 'epoch': 0.73}\n",
      "{'loss': 2.5249, 'grad_norm': 2.1326076984405518, 'learning_rate': 3.694516971279374e-05, 'epoch': 0.78}\n",
      "{'loss': 2.4319, 'grad_norm': 2.2466139793395996, 'learning_rate': 3.607484769364665e-05, 'epoch': 0.84}\n",
      "{'loss': 2.4028, 'grad_norm': 2.27268385887146, 'learning_rate': 3.5204525674499566e-05, 'epoch': 0.89}\n",
      "{'loss': 2.4307, 'grad_norm': 2.0834062099456787, 'learning_rate': 3.4334203655352484e-05, 'epoch': 0.94}\n",
      "{'loss': 2.3751, 'grad_norm': 2.7267560958862305, 'learning_rate': 3.3463881636205394e-05, 'epoch': 0.99}\n",
      "{'loss': 2.2961, 'grad_norm': 2.4425160884857178, 'learning_rate': 3.259355961705831e-05, 'epoch': 1.04}\n",
      "{'loss': 2.2997, 'grad_norm': 2.503073215484619, 'learning_rate': 3.172323759791123e-05, 'epoch': 1.1}\n",
      "{'loss': 2.3205, 'grad_norm': 2.8083813190460205, 'learning_rate': 3.085291557876415e-05, 'epoch': 1.15}\n",
      "{'loss': 2.3341, 'grad_norm': 3.4645869731903076, 'learning_rate': 2.998259355961706e-05, 'epoch': 1.2}\n",
      "{'loss': 2.2767, 'grad_norm': 3.019308567047119, 'learning_rate': 2.9112271540469975e-05, 'epoch': 1.25}\n",
      "{'loss': 2.2793, 'grad_norm': 2.9355216026306152, 'learning_rate': 2.8241949521322892e-05, 'epoch': 1.31}\n",
      "{'loss': 2.3402, 'grad_norm': 2.1840672492980957, 'learning_rate': 2.7371627502175807e-05, 'epoch': 1.36}\n",
      "{'loss': 2.2614, 'grad_norm': 2.834792137145996, 'learning_rate': 2.650130548302872e-05, 'epoch': 1.41}\n",
      "{'loss': 2.2777, 'grad_norm': 2.131422281265259, 'learning_rate': 2.5630983463881635e-05, 'epoch': 1.46}\n",
      "{'loss': 2.3067, 'grad_norm': 2.1658248901367188, 'learning_rate': 2.4760661444734552e-05, 'epoch': 1.51}\n",
      "{'loss': 2.2557, 'grad_norm': 1.8609586954116821, 'learning_rate': 2.389033942558747e-05, 'epoch': 1.57}\n",
      "{'loss': 2.2829, 'grad_norm': 2.371381998062134, 'learning_rate': 2.3020017406440384e-05, 'epoch': 1.62}\n",
      "{'loss': 2.2817, 'grad_norm': 2.574915647506714, 'learning_rate': 2.2149695387293298e-05, 'epoch': 1.67}\n",
      "{'loss': 2.3289, 'grad_norm': 2.5624043941497803, 'learning_rate': 2.1279373368146216e-05, 'epoch': 1.72}\n",
      "{'loss': 2.3061, 'grad_norm': 3.175842761993408, 'learning_rate': 2.040905134899913e-05, 'epoch': 1.78}\n",
      "{'loss': 2.2241, 'grad_norm': 2.0701019763946533, 'learning_rate': 1.9538729329852047e-05, 'epoch': 1.83}\n",
      "{'loss': 2.2627, 'grad_norm': 2.1138312816619873, 'learning_rate': 1.866840731070496e-05, 'epoch': 1.88}\n",
      "{'loss': 2.2751, 'grad_norm': 1.8871976137161255, 'learning_rate': 1.779808529155788e-05, 'epoch': 1.93}\n",
      "{'loss': 2.2511, 'grad_norm': 2.238740921020508, 'learning_rate': 1.6927763272410793e-05, 'epoch': 1.98}\n",
      "{'loss': 2.2289, 'grad_norm': 2.88749098777771, 'learning_rate': 1.6057441253263707e-05, 'epoch': 2.04}\n",
      "{'loss': 2.1438, 'grad_norm': 2.1909983158111572, 'learning_rate': 1.5187119234116623e-05, 'epoch': 2.09}\n",
      "{'loss': 2.2253, 'grad_norm': 2.8424477577209473, 'learning_rate': 1.431679721496954e-05, 'epoch': 2.14}\n",
      "{'loss': 2.205, 'grad_norm': 2.1060473918914795, 'learning_rate': 1.3446475195822456e-05, 'epoch': 2.19}\n",
      "{'loss': 2.1342, 'grad_norm': 2.470076322555542, 'learning_rate': 1.257615317667537e-05, 'epoch': 2.25}\n",
      "{'loss': 2.1983, 'grad_norm': 2.153205156326294, 'learning_rate': 1.1705831157528286e-05, 'epoch': 2.3}\n",
      "{'loss': 2.1877, 'grad_norm': 2.4362475872039795, 'learning_rate': 1.0835509138381202e-05, 'epoch': 2.35}\n",
      "{'loss': 2.2092, 'grad_norm': 2.0181350708007812, 'learning_rate': 9.965187119234116e-06, 'epoch': 2.4}\n",
      "{'loss': 2.1948, 'grad_norm': 2.691279649734497, 'learning_rate': 9.094865100087033e-06, 'epoch': 2.45}\n",
      "{'loss': 2.1935, 'grad_norm': 3.166893720626831, 'learning_rate': 8.224543080939948e-06, 'epoch': 2.51}\n",
      "{'loss': 2.231, 'grad_norm': 2.375030755996704, 'learning_rate': 7.354221061792864e-06, 'epoch': 2.56}\n",
      "{'loss': 2.2524, 'grad_norm': 1.9678966999053955, 'learning_rate': 6.483899042645779e-06, 'epoch': 2.61}\n",
      "{'loss': 2.1863, 'grad_norm': 2.393047332763672, 'learning_rate': 5.613577023498695e-06, 'epoch': 2.66}\n",
      "{'loss': 2.2215, 'grad_norm': 1.8616933822631836, 'learning_rate': 4.743255004351611e-06, 'epoch': 2.72}\n",
      "{'loss': 2.1858, 'grad_norm': 2.3481638431549072, 'learning_rate': 3.872932985204526e-06, 'epoch': 2.77}\n",
      "{'loss': 2.1482, 'grad_norm': 2.145042896270752, 'learning_rate': 3.0026109660574416e-06, 'epoch': 2.82}\n",
      "{'loss': 2.164, 'grad_norm': 2.3108632564544678, 'learning_rate': 2.132288946910357e-06, 'epoch': 2.87}\n",
      "{'loss': 2.1775, 'grad_norm': 2.227491855621338, 'learning_rate': 1.2619669277632725e-06, 'epoch': 2.92}\n",
      "{'loss': 2.2312, 'grad_norm': 2.703258991241455, 'learning_rate': 3.91644908616188e-07, 'epoch': 2.98}\n",
      "{'train_runtime': 101413.9819, 'train_samples_per_second': 0.227, 'train_steps_per_second': 0.057, 'train_loss': 2.3460681593241746, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model\\\\tokenizer_config.json',\n",
       " './fine_tuned_model\\\\special_tokens_map.json',\n",
       " './fine_tuned_model\\\\vocab.json',\n",
       " './fine_tuned_model\\\\merges.txt',\n",
       " './fine_tuned_model\\\\added_tokens.json',\n",
       " './fine_tuned_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Load your data with pandas\n",
    "df = pd.read_csv(\"bc_incidence_full_text_31jan25.csv\")\n",
    "# Suppose you want to combine all text columns into one\n",
    "df[\"combined_text\"] = df.astype(str).agg(\" \".join, axis=1)\n",
    "corpus = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 2. Initialize your tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Tokenize the corpus\n",
    "tokenized_output = tokenizer(corpus, truncation=True, max_length=512)\n",
    "\n",
    "# 4. Convert tokenized output to a Dataset\n",
    "train_dataset = Dataset.from_dict(tokenized_output)\n",
    "\n",
    "# 5. Set up the data collator for language modeling (mlm=False for causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 6. Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"no\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# 7. Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 8. Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# 9. Save the fine-tuned model and tokenizer\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
